## Prebuilt ReAct agent vs. tool‑only memory capture: what went wrong and how we fixed it

### TL;DR

- **Problem**: The prebuilt ReAct agent emits assistant chat and can loop tool calls, which polluted state and caused duplicate `add_entry` with role mixups.
- **Fix**: Replace the prebuilt loop with a **custom, tool‑only** LangGraph workflow that:
  - Feeds the model only system + conversation messages (user/assistant).
  - Captures only `tool_calls` (no assistant text) from the model.
  - Executes tools via a `ToolNode`; only `add_entry` persists to external memory.

---

## Context & invariants

We observe a conversation and persist it to Mycelian Memory via MCP tools.

- **Invariant**: One conversation message → exactly one `add_entry`.
- Persist the **correct role** (`user` or `assistant`) and the original **content verbatim**.
- **System** messages (`SESSION_START`, `FLUSH_CONTEXT`, `SESSION_END`) only drive control tools; they are **never** persisted.

---

## Symptoms we saw

From per‑question logs (abridged):

```text
STATE_MSG idx=8  type=ai  tool_calls=1[add_entry]
STATE_MSG idx=10 type=ai  tool_calls=1[add_entry]      # duplicate for same turn
STATE_MSG idx=12 type=ai  content="Here are solo‑friendly destinations..."  # agent-authored assistant text
```

Also observed:

- Duplicate `add_entry` attempts for the same turn (`msg_idx`).
- Assistant text generated by the agent itself appearing in state.
- Role/content mismatches (assistant turns persisted as user).
- Redundant context bootstrap (`get_context`/`list_entries` twice at `SESSION_START`).
- Missing flushes (`await_consistency`/`put_context`) at expected times.

---

## Root causes

- **Prebuilt ReAct agents are autonomous loops**: the LLM both decides tools and emits assistant content. Those AI messages get appended to state, polluting context and encouraging tool retries.
- **Prompt-only constraints are insufficient**: even with strong guidance, the ReAct loop still produced assistant chat and duplicate `add_entry`.
- **Role disambiguation was weak**: the model conflated the conversation assistant with the agent (our tool runner), leading to incorrect tags and duplicates.

---

## Mitigation: custom tool‑only LangGraph workflow

We implemented a custom graph modeled after LangGraph’s agent pattern, adapted to be **tool‑only** so the model never emits assistant text.

Reference: LangGraph “Workflows & agents → Agent” guide ([link](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent)).

### Key ideas

- **Model input curation**: prepend our system prompt and pass only `SystemMessage` + `ChatMessage` (role=user/assistant) to the LLM.
- **Tool‑calls only**: record only the LLM’s `tool_calls` (store as `AIMessage(content="", tool_calls=...)`). Drop any free‑form assistant text.
- **Tool execution**: `ToolNode(self._agent_tools)` runs the tools, appending `ToolMessage` results for the loop only; these are not fed back into the LLM input.
- **Checkpointed per‑thread state**: compile with `InMemorySaver`, invoke with `{"configurable": {"thread_id": <id>}}`, and send only the new message each turn.
- **Prompt enforcement (concise)**:
  - Identity: You are “mycelian”; never persist role=mycelian.
  - Output: tools only; never emit assistant chat.
  - Conversation: exactly one `add_entry` per `(thread_id,msg_idx)`; verbatim content; tags `{role,msg_idx}`.
  - System: `SESSION_START → get_context` once then `list_entries(limit=10)`; `FLUSH_CONTEXT/SESSION_END → await_consistency` then `put_context`. System messages are never persisted.
  - Never call `get_context`/`list_entries` during conversation.
  - Optional: periodic flush at `msg_idx % 6 == 0`.

### Minimal implementation sketch (Python)

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import SystemMessage, ChatMessage, AIMessage, ToolMessage

# 1) Bind tools to the model
llm_with_tools = self._llm.bind_tools(self._agent_tools)

def _curate_for_model(msgs):
    return [m for m in msgs if isinstance(m, (SystemMessage, ChatMessage))]

def llm_call(state: MessagesState):
    model_input = [SystemMessage(content=prompt_with_ids)] + _curate_for_model(state["messages"])
    resp = llm_with_tools.invoke(model_input)
    # Record tool-calls only; no assistant text stored in state
    return {"messages": [AIMessage(content="", tool_calls=getattr(resp, "tool_calls", []),
                                    additional_kwargs=getattr(resp, "additional_kwargs", {}))]}

def should_continue(state: MessagesState):
    last = state["messages"][-1]
    return "tools" if getattr(last, "tool_calls", None) else END

builder = StateGraph(MessagesState)
builder.add_node("llm_call", llm_call)
builder.add_node("tools", ToolNode(self._agent_tools))
builder.add_edge(START, "llm_call")
builder.add_conditional_edges("llm_call", should_continue, {"tools": "tools", END: END})
builder.add_edge("tools", "llm_call")

self._agent = builder.compile(checkpointer=self._checkpointer)

# Per-turn adapter: send only the new message; state accrues per thread_id
def invoke_message(type: str, content: str, thread_id: str, role: str | None, msg_idx: int | None):
    msg = SystemMessage(content=content) if type == "system" else ChatMessage(role=role, content=content)
    return self._agent.invoke({"messages": [msg]}, {"configurable": {"thread_id": thread_id}})
```

---

## Before vs After: state shape

### Before (polluted by prebuilt agent)

- `SystemMessage("SESSION_START")`
- `AIMessage(tool_calls=[get_context])`; `ToolMessage(context)`
- `AIMessage(tool_calls=[list_entries])`; `ToolMessage(entries)`
- `ChatMessage(role=user, "…")`
- `AIMessage(tool_calls=[add_entry])`; `ToolMessage("enqueued")`
- `AIMessage(content="Here are destinations…")`  ← BAD: agent-authored assistant text
- `AIMessage(tool_calls=[add_entry])`             ← BAD: duplicate persistence

### After (tool‑only)

- `SystemMessage("SESSION_START")`
- `AIMessage(tool_calls=[get_context])`; `ToolMessage(context)`
- `AIMessage(tool_calls=[list_entries])`; `ToolMessage(entries)`
- `ChatMessage(role=user, "…")`
- `AIMessage(tool_calls=[add_entry])`; `ToolMessage("enqueued")`
- `ChatMessage(role=assistant, "…")`
- `AIMessage(tool_calls=[add_entry])`; `ToolMessage("enqueued")`
- Optional periodic flush: `AIMessage(tool_calls=[await_consistency, put_context])` + two `ToolMessage("ok")`

No assistant text authored by the agent is ever added to state; AIMessage entries contain tool_calls only.

---

## Validation checklist

- No model‑authored assistant text appears in `STATE_MSG` lines.
- Exactly one `add_entry` per `(thread_id,msg_idx)`.
- `SESSION_START` performs a single `get_context` and `list_entries(limit=10)`.
- Optional periodic flush occurs at `msg_idx % 6 == 0` (prompt‑driven or via a tiny programmatic node).
- Question logs show clean `TOOL_CALL` lines and consistent `STATE_MSG` audits.

---

## Tradeoffs & next steps

- **Tradeoff**: We sacrificed visible assistant “reasoning traces” in state in favor of a deterministic, auditable tool‑only loop—ideal for memory capture pipelines.
- **Option**: Add a programmatic flush node keyed off `msg_idx` instead of relying solely on prompt guidance.
- **Option**: Keep a defensive dedupe guard in code for `add_entry` keyed by `(thread_id, role, msg_idx)`.

---

## References

- LangGraph – Workflows & agents → Agent (custom graph with tool loop): [link](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent)
